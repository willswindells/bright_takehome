#! /usr/bin/env -S uv run --script

# /// script
# dependencies = [
#     "dbt-core>=1.9.3",
#     "dbt-duckdb>=1.9.2",
#     "duckdb>=1.2.1",
#     "pandas>=2.2.3",
# ]
# ///

import subprocess
import threading
import sys
import json
import os
import time
import duckdb


def db_import_data():
    """
    updates via simple replace the seed table for raw bluesky data
    One run a minuet loop as streaming example
    """

    print("attempting connection to duckdb")
    conn = duckdb.connect("bluesky.db")
    print("connected to duckDB")

    # first batch buildup
    time.sleep(5)

    # time loop delay
    while True:
        print("Starting load")
        conn.execute("""
        CREATE OR REPLACE TABLE seed_bluesky_raw AS
        SELECT *
        FROM 'bluesky.json';
        -- FROM read_json_auto('bluesky.json');
        """)
        print("loaded_raw_file")
        time.sleep(6)

def append_json_to_file(json_string, filename="bluesky.json"):
    """
    Parses a single-line JSON string and appends it to a file, one JSON object per line.

    Args:
        json_string (str): The single-line JSON string.
        filename (str): The name of the file to append to.
    """

    try:
        data = json.loads(json_string)
        with open(filename, "a") as f: 
            json.dump(data, f)
            f.write("\n") 
        print(f"JSON data appended to {filename}")
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

def db_readout():
    """
    Reads JSON data from stdin and saves it to a file.
    """

    print("python listening\n")
    for line in sys.stdin:
        print("\nline ")
        append_json_to_file(line)

    print("python output\n")
 

def db_dbt_run():
    """
    runs dbt build every 50 seconds on duckdb
    """
    print("attempting dbt build")
    # time loop delay
    while True:
        subprocess.call("dbt_build.sh")
        print("dbt build done")
        time.sleep(50)

def run_and_export_csvs():
    """
    runs extracting csvs answers 50 second loop
    """
    print("extracting data")
    conn = duckdb.connect("bluesky.db")
    # time loop delay
    while True:
        conn.execute("""COPY engineering_post_analysis TO 'engineering_related_words.csv' (HEADER, DELIMITER ',')""")
        conn.execute("""COPY likes-per-minutes TO 'likes-per-minutes.csv' (HEADER, DELIMITER ',')""")
        conn.execute("""COPY fast-likers TO 'fast-likers.csv' (HEADER, DELIMITER ',')""")
        time.sleep(50)


def run_capture_and_import_and_dbt_build():
    """Runs capture and import alongside eachother"""

    readout = threading.Thread(target=db_readout) 
    import_data = threading.Thread(target=db_import_data) 
    dbt_build = threading.Thread(target=db_dbt_run)
    csvs_build = threading.Thread(target=run_and_export_csvs) 

    readout.start()
    import_data.start()
    dbt_build.start()
    csvs_build.start()

    readout.join() 
    import_data.join() 
    dbt_build.join() 
    csvs_build.join() 

    print("Done")


def main():

    #New to multithreading
    run_capture_and_import_and_dbt_build()


if __name__ == "__main__":
    main()